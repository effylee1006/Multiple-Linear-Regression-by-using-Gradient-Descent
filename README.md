# Multiple-Linear-Regression-by-using-Gradient-Descent
## 內容
- 1 緒論
  - 1.1線性回歸的定義
  - 1.2單變量線性回歸
  - 1.3多變量線性回歸

- 2梯度下降
  - 2.1 cost function
  - 2.2 梯度下降：解決線性回歸的方法之一
  - 2.3 feature scaling：加快梯度下降執行速度的方法
  - 2.4 梯度下降的實現

- 3 梯度下降的擴展和比較

## 1 緒論
### 1.1線性回歸的定義
線性回歸，是利用數理統計中回歸分析，來確定兩種或兩種以上變量間相互依賴的定量關係的一種統計分析方法，運用十分廣泛。其表達形式為y = w'x+e，e為誤差服從均值為0的正態分佈。
具體來說，如果輸入x是列向量，目標y 是連續值（實數或連續整數），預測函數f(x)的輸出也是連續值。這種機器學習問題是回歸問題。如果我們定義f(x)是線性函數，f(x) = wTx + b, 這就是線性回歸問題（Linear Regression）。

### 1.2單變量線性回歸
單變量線性回歸就是從一個輸入值預測一個輸出值。輸入/輸出的對應關係就是一個線性函數。

### 1.3多變量線性回歸
多變量線性回歸就是說輸出值受多個輸入變量x影響，每個變量的影響大小用w(weight)來刻畫，w與x呈線性關係，然後把多個wx線性組合(w1x1,w2x2,,,)相加，再和輸出y的對應關係就是一個多變量線性回歸問題。注意：多變量線性回歸不是指y與X是線性的，而是指各自的w與x呈線性關係。所以最終的結果，在幾何上甚至可能呈現出曲線，而不是必須是直線。

## 2 梯度下降
### 2.1 cost function
線性回歸屬於監督學習，因此方法和監督學習應該是一樣的，先給定一個訓練集，根據這個訓練集學習出一個線性函數，然後測試這個函數訓練的好不好（即此函數是否足夠擬合訓練集數據），挑選出最好的函數（cost function最小）即可。

3)Cost Function的用途：對假設的函數進行評價，cost function越小的函數，說明擬合訓練數據擬合的越好。
下圖詳細說明了當cost function為黑盒的時候，cost function 的作用。

![image](https://user-images.githubusercontent.com/97221948/149249571-853f78a5-b829-46ed-9f2b-aedb057a9caa.png)
![image](https://user-images.githubusercontent.com/97221948/149249662-41ed7114-f723-4c81-8609-ac5fee35047f.png)

### 2.2 梯度下降：解決線性回歸的方法之一
但是又一個問題引出了，雖然給定一個函數，我們能夠根據cost function知道這個函數擬合的好不好，但是畢竟函數有這麼多，總不可能一個一個試吧？因此我們引出了梯度下降：能夠找出cost function函數的最小值；梯度下降原理：將函數比作一座山，我們站在某個山坡上，往四周看，從哪個方向向下走一小步，能夠下降的最快。當然解決問題的方法有很多，梯度下降只是其中一個，還有一種方法叫Normal Equation。

方法：
(1)先確定向下一步的步伐大小，我們稱為Learning rate；
(2)任意給定一個初始值：這裡寫圖片描述 這裡寫圖片描述；
(3)確定一個向下的方向，並向下走預先規定的步伐，並更新這裡寫圖片描述 這裡寫圖片描述；
(4)當下降的高度小於某個定義的值，則停止下降。

特點：
(1)初始點不同，獲得的最小值也不同，因此梯度下降求得的只是局部最小值；
(2)越接近最小值時，下降速度越慢。

如果Learning rate取值後發現J function 增長了，則需要減小Learning rate的值。
因此我們能夠對cost function運用梯度下降，即將梯度下降和線性回歸進行整合
![image](https://user-images.githubusercontent.com/97221948/149250048-685190fd-32b7-411f-8470-a6d7e7df593f.png)


————————————————
版权声明：本文为CSDN博主「Robin_just」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/shaguabufadai/article/details/72858293
